#version: '3'

services:
  devcontainer:
    build: 
      context: .
      dockerfile: Dockerfile

    volumes:
      # Forwards the local Docker socket to the container.
      - /var/run/docker.sock:/var/run/docker-host.sock 
      # Update this to wherever you want VS Code to mount the folder of your project
      - ../..:/workspaces:cached

    # Overrides default command so things don't shut down after the process ends.
    entrypoint: /usr/local/share/docker-init.sh
    command: sleep infinity 

    # Uncomment the next four lines if you will use a ptrace-based debuggers like C++, Go, and Rust.
    # cap_add:
    #  - SYS_PTRACE
    # security_opt:
    #   - seccomp:unconfined

    # Use "forwardPorts" in **devcontainer.json** to forward an app port locally. 
    # (Adding the "ports" property to this file will not forward from a Codespace.)

  config-init:
    image: alpine:latest
    container_name: pgadmin_config_init
    env_file: .env
    volumes:
      - pgadmin_configs:/shared
    entrypoint: 
      - /bin/sh
      - -c
      - |
        # 1. Create the pgpass file inside the shared volume
        # Use a standard filename that pgAdmin expects
        echo "postgres:5432:${POSTGRES_DB}:${POSTGRES_USER}:${POSTGRES_PASSWORD}" > /shared/pgpass
        echo "${POSTGRES_PASSWORD}" > /shared/pgpass
        # chmod 644 /shared/pgpass
        #chown 5050:5050 /shared/pgpass

        # 2. Update servers.json to point to the correct pgpass location
        cat <<EOF > /shared/servers.json
        {
          "Servers": {
            "1": {
              "Name": "Multi-Agent-DB",
              "Group": "Servers",
              "Host": "postgres",
              "Port": 5432,
              "MaintenanceDB": "${POSTGRES_DB}",
              "Username": "${POSTGRES_USER}",
              "SSLMode": "prefer",
              "PassFile": "/shared/pgpass"
            }
          }
        }
        EOF
        echo "Configuration successfully generated for stack."

  pgadmin:
    image: dpage/pgadmin4:latest
    container_name: pgadmin
    restart: unless-stopped
    ports: ["8082:80"]
    env_file: .env
    environment:
      PGADMIN_DEFAULT_EMAIL: ${PGADMIN_DEFAULT_EMAIL}
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_DEFAULT_PASSWORD}
      # Point directly to the location in the shared volume
      PGADMIN_SERVER_JSON_FILE: /shared/servers.json
      PGADMIN_REPLACE_SERVERS_ON_STARTUP: 'True'
      PGADMIN_CONFIG_MASTER_PASSWORD_REQUIRED: 'False'
    volumes:
      - pgadmin_data:/var/lib/pgadmin
      - pgadmin_configs:/shared
      # Directly mount pgpass into the user-specific storage path
      # Note: replace @ with _ in your email for the path
      #- pgadmin_configs:/var/lib/pgadmin/storage/admin_example.com/
    depends_on:
      config-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles: ["optional"]
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      # Option A: Fastest check using native ollama binary
      #test: ["CMD", "ollama", "--version"]
      
      # Option B: Network-based check (useful if you need to confirm the API is listening)
      test: ["CMD-SHELL", "cat < /dev/tcp/localhost/11434 || exit 1"]
      
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s # Allows time for heavy model loading on startup

  langtrace:
    image: scale3labs/langtrace-client:latest
    restart: always
    container_name: langtrace
    profiles: ["optional"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "3001:3000"
    env_file: .env
    healthcheck:
      # Checks if the web UI/API is responding
      test: ["CMD", "curl", "-f", "http://localhost:3000/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  litellm:
    image: docker.litellm.ai/berriai/litellm:main-stable
    container_name: litellm
    restart: unless-stopped
    ports:
      - "4000:4000"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      DATABASE_URL: ${POSTGRES_URL}
      STORE_MODEL_IN_DB: "True"
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY}
      LANGFUSE_PUBLIC_KEY: ${LANGFUSE_PUBLIC_KEY}
      LANGFUSE_SECRET_KEY: ${LANGFUSE_SECRET_KEY}
      LANGFUSE_HOST: ${LANGFUSE_HOST}
      PROMETHEUS_METRICS: "True"
      OLLAMA_API_BASE: ${OLLAMA_API_BASE}
    env_file:
      - .env
    volumes:
      - ./litellm_config.yaml:/app/config.yaml
    command: ["--config", "/app/config.yaml"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:4000/health/readiness"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s

  prometheus:
    image: prom/prometheus
    container_name: prometheus
    restart: unless-stopped
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9092:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=15d"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090"] # Checks if the Prometheus UI is reachable
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 15s


volumes:
  postgres_data:
  pgadmin_data:
  pgadmin_configs:
  prometheus_data:
  redis_data:
  ollama_data:
